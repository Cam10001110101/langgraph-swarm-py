{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! pip install langchain-mcp-adapters langgraph \"langchain[anthropic]\" langgraph-swarm httpx markdownify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm\n",
    "\n",
    "planner_prompt = \"\"\"\n",
    "<Task>\n",
    "You will help plan the steps to implement a LangGraph application based on the user's request. \n",
    "</Task>\n",
    "\n",
    "<Instructions>\n",
    "1. Reflect on the user's request. \n",
    "2. Use the fetch_doc tool to read this llms.txt file: {llms_txt}\n",
    "3. Identify documents that are relevant to the user's request.\n",
    "4. Ask any follow-up questions to help refine the project scope and narrow the set of documents to be used for the project.\n",
    "5. When the project scope is clear produce two things: \n",
    "- A short description that lays out the scope of the project \n",
    "- A list of relevant URLs to reference in order to implement the project\n",
    "6. Finally, transfer to the research agent using the transfer_to_researcher_agent tool.\n",
    "</Instructions>\n",
    "\"\"\"\n",
    "\n",
    "researcher_prompt = \"\"\"\n",
    "<Task>\n",
    "You will perform research using provided URLs and use these to implement the solution to the user's request. \n",
    "</Task>\n",
    "\n",
    "<Instructions>\n",
    "1. Reflect on the project scope and provided URLs.\n",
    "2. Use the fetch_doc tool to fetch and read each URL.\n",
    "3. Use the information in these URLs to implement the solution to the user's request.\n",
    "4. If you need further clarification or additional sources to implement the solution, then transfer to transfer_to_planner_agent.\n",
    "</Instructions>\n",
    "\"\"\"\n",
    "\n",
    "# LLM\n",
    "model = ChatAnthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "# Handoff tools\n",
    "transfer_to_planner_agent = create_handoff_tool(\n",
    "    agent_name=\"planner_agent\",\n",
    "    description=\"Transfer the user to the planner_agent for clarifying questions related to the user's request.\"\n",
    ")\n",
    "transfer_to_researcher_agent = create_handoff_tool(\n",
    "    agent_name=\"researcher_agent\",\n",
    "    description=\"Transfer the user to the researcher_agent to perform research and implement the solution to the user's request.\"\n",
    ")\n",
    "\n",
    "# LLMS.txt\n",
    "llms_txt = \"LangGraph:https://langchain-ai.github.io/langgraph/llms.txt\"\n",
    "planner_prompt_formatted = planner_prompt.format(llms_txt=llms_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from markdownify import markdownify\n",
    "\n",
    "httpx_client = httpx.Client(follow_redirects=False, timeout=10)\n",
    "\n",
    "def fetch_doc(url: str) -> str:\n",
    "        \"\"\" Fetch a document from a URL and return the markdownified text. \n",
    "        Args:\n",
    "            url (str): The URL of the document to fetch.\n",
    "        Returns:\n",
    "            str: The markdownified text of the document.\n",
    "        \"\"\"\n",
    "   \n",
    "        try:\n",
    "            response = httpx_client.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return markdownify(response.text)\n",
    "        except (httpx.HTTPStatusError, httpx.RequestError) as e:\n",
    "            return f\"Encountered an HTTP error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "planner_agent = create_react_agent(model,\n",
    "                                   prompt=planner_prompt_formatted, \n",
    "                                   tools=[fetch_doc,transfer_to_researcher_agent],\n",
    "                                   name=\"planner_agent\") \n",
    "\n",
    "# Researcher agent\n",
    "researcher_agent = create_react_agent(model, \n",
    "                                    prompt=researcher_prompt, \n",
    "                                    tools=[fetch_doc,transfer_to_researcher_agent],\n",
    "                                    name=\"researcher_agent\") \n",
    "\n",
    "# Swarm\n",
    "checkpointer = InMemorySaver()\n",
    "agent_swarm = create_swarm([planner_agent, researcher_agent], default_active_agent=\"planner_agent\")\n",
    "app = agent_swarm.compile(checkpointer=checkpointer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input \n",
    "request =  \"Create a LangGraph application that is a prompt chain: it takes a topic from a user, generates a joke, and checks if the joke has a punchline.\"\n",
    "messages = {\"role\": \"user\", \"content\": request}\n",
    "response = app.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MCP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import StdioServerParameters\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"uvx\",\n",
    "    args=[\n",
    "            \"--from\",\n",
    "            \"mcpdoc\",\n",
    "            \"mcpdoc\",\n",
    "            \"--urls\",\n",
    "            \"LangGraph:https://langchain-ai.github.io/langgraph/llms.txt\",\n",
    "            \"--transport\",\n",
    "            \"stdio\",\n",
    "            \"--port\",\n",
    "            \"8081\",\n",
    "            \"--host\",\n",
    "            \"localhost\"\n",
    "            ],\n",
    ")\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # Initialize the connection\n",
    "        await session.initialize()\n",
    "\n",
    "        # Get tools\n",
    "        tools = await load_mcp_tools(session)\n",
    "\n",
    "        planner_agent = create_react_agent(model,\n",
    "                                        prompt=planner_prompt, \n",
    "                                        tools=tools.append(transfer_to_researcher_agent),\n",
    "                                        name=\"planner_agent\") \n",
    "\n",
    "        # Researcher agent\n",
    "        researcher_agent = create_react_agent(model, \n",
    "                                            prompt=researcher_prompt, \n",
    "                                            tools=tools.append(transfer_to_planner_agent),\n",
    "                                            name=\"researcher_agent\") \n",
    "\n",
    "        # Swarm\n",
    "        agent_swarm = create_swarm([planner_agent, researcher_agent], default_active_agent=\"planner_agent\")\n",
    "\n",
    "        # app = agent_swarm.compile(config_schema=Configuration)\n",
    "        agent = agent_swarm.compile()\n",
    "        response = await agent.ainvoke({\"messages\": messages})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "for m in agent_response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Research and planning tools \n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"planning-server\": {\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "            \"--from\",\n",
    "            \"mcpdoc\",\n",
    "            \"mcpdoc\",\n",
    "            \"--urls\",\n",
    "            llms_txt_urls,\n",
    "            \"--transport\",\n",
    "            \"stdio\",\n",
    "            \"--port\",\n",
    "            \"8081\",\n",
    "            \"--host\",\n",
    "            \"localhost\"\n",
    "            ],\n",
    "            \"transport\": \"stdio\",\n",
    "        }\n",
    "    }\n",
    "\n",
    ") as client:\n",
    "    # Planner agent\n",
    "    planner_agent = create_react_agent(model,\n",
    "                                       prompt=planner_prompt, \n",
    "                                       tools=client.server_name_to_tools[\"planning-server\"].append(transfer_to_researcher_agent),\n",
    "                                       name=\"planner_agent\") \n",
    "\n",
    "    # Researcher agent\n",
    "    researcher_agent = create_react_agent(model, \n",
    "                                          prompt=researcher_prompt, \n",
    "                                          tools=client.server_name_to_tools[\"planning-server\"].append(transfer_to_planner_agent),\n",
    "                                          name=\"researcher_agent\") \n",
    "\n",
    "    # Swarm\n",
    "    agent_swarm = create_swarm([planner_agent, researcher_agent], default_active_agent=\"planner_agent\")\n",
    "\n",
    "    # app = agent_swarm.compile(config_schema=Configuration)\n",
    "    agent = agent_swarm.compile()\n",
    "    response = await agent.ainvoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "        \"research-server\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\"@playwright/mcp\"],\n",
    "            \"transport\": \"stdio\",\n",
    "            \"env\": {\n",
    "                \"PATH\": \"/Users/rlm/.cursor/extensions/ms-python.python-2024.12.3-darwin-arm64/python_files/deactivate/zsh:/Users/rlm/Desktop/Code/langgraph-swarm/.venv/bin:/Users/rlm/.bun/bin:/Users/rlm/.poetry/bin:/Users/rlm/Library/Python/3.13/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Users/rlm/.cargo/bin:/Users/rlm/miniforge3/condabin:/Users/rlm/.local/bin\"\n",
    "            }\n",
    "        },\n",
    "        \"planning-server\": {\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "            \"--from\",\n",
    "            \"mcpdoc\",\n",
    "            \"mcpdoc\",\n",
    "            \"--urls\",\n",
    "            llms_txt_urls,\n",
    "            \"--transport\",\n",
    "            \"stdio\",\n",
    "            \"--port\",\n",
    "            \"8081\",\n",
    "            \"--host\",\n",
    "            \"localhost\"\n",
    "            ],\n",
    "            \"transport\": \"stdio\",\n",
    "        }\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
